{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0fbb05950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(X_train[i], cmap='gray')\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale [0, 255] -> [0, 1]\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer-valued labes\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "Encoded labes\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels\n",
    "from keras.utils import np_utils\n",
    "print ('Integer-valued labes')\n",
    "print (y_train[:10])\n",
    "\n",
    "# One-hot encode\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print ('Encoded labes')\n",
    "print (y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 12.44%\n"
     ]
    }
   ],
   "source": [
    "# We use categorical cross entropy for multi-class classification\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate on test set pre-training\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.2802 - acc: 0.9127 - val_loss: 0.1152 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11516, saving model to mnist.model.best.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 115us/step - loss: 0.1131 - acc: 0.9646 - val_loss: 0.1014 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11516 to 0.10139, saving model to mnist.model.best.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.0787 - acc: 0.9757 - val_loss: 0.0912 - val_acc: 0.9718\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10139 to 0.09118, saving model to mnist.model.best.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.0658 - acc: 0.9798 - val_loss: 0.0914 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09118\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.0523 - acc: 0.9840 - val_loss: 0.0771 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09118 to 0.07708, saving model to mnist.model.best.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.0453 - acc: 0.9860 - val_loss: 0.1005 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07708\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.0396 - acc: 0.9881 - val_loss: 0.0984 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07708\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 8s 160us/step - loss: 0.0346 - acc: 0.9900 - val_loss: 0.1013 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07708\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.0322 - acc: 0.9907 - val_loss: 0.1060 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07708\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.0281 - acc: 0.9919 - val_loss: 0.1062 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07708\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.17%\n"
     ]
    }
   ],
   "source": [
    "# Load model with best weights\n",
    "model.load_weights('mnist.model.best.hdf5')\n",
    "\n",
    "# Evaluate the trained model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2048)              1607680   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 5,824,522\n",
      "Trainable params: 5,824,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 43s 891us/step - loss: 0.2675 - acc: 0.9214 - val_loss: 0.1160 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11597, saving model to mnist.model.overfit.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 43s 893us/step - loss: 0.1106 - acc: 0.9690 - val_loss: 0.1410 - val_acc: 0.9606\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.11597\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 43s 896us/step - loss: 0.0814 - acc: 0.9766 - val_loss: 0.1014 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11597 to 0.10142, saving model to mnist.model.overfit.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 38s 795us/step - loss: 0.0707 - acc: 0.9803 - val_loss: 0.1383 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10142\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 29s 605us/step - loss: 0.0648 - acc: 0.9833 - val_loss: 0.1574 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10142\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 28s 592us/step - loss: 0.0564 - acc: 0.9865 - val_loss: 0.1613 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10142\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 28s 590us/step - loss: 0.0530 - acc: 0.9877 - val_loss: 0.1284 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10142\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 28s 584us/step - loss: 0.0481 - acc: 0.9883 - val_loss: 0.1505 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10142\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 28s 584us/step - loss: 0.0423 - acc: 0.9902 - val_loss: 0.1264 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10142\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 28s 586us/step - loss: 0.0425 - acc: 0.9902 - val_loss: 0.1556 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10142\n",
      "Test Accuracy: 97.81%\n"
     ]
    }
   ],
   "source": [
    "# Increase the number of nodes in hidden layers to see if the model overfits\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.overfit.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more nodes in hidden layers clearly led to overfitting, as seen by the increase in validation loss, decrease in training loss, and improvements in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.5708 - acc: 0.8283 - val_loss: 0.2131 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21308, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2748 - acc: 0.9201 - val_loss: 0.1654 - val_acc: 0.9515\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21308 to 0.16537, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2146 - acc: 0.9359 - val_loss: 0.1325 - val_acc: 0.9599\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16537 to 0.13250, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1855 - acc: 0.9450 - val_loss: 0.1212 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13250 to 0.12119, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1648 - acc: 0.9508 - val_loss: 0.1117 - val_acc: 0.9663\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12119 to 0.11167, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1523 - acc: 0.9549 - val_loss: 0.1051 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11167 to 0.10511, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1379 - acc: 0.9581 - val_loss: 0.1040 - val_acc: 0.9702\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10511 to 0.10401, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1337 - acc: 0.9609 - val_loss: 0.0989 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10401 to 0.09895, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1262 - acc: 0.9619 - val_loss: 0.0973 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09895 to 0.09729, saving model to mnist.model.underfit.hdf5\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.1204 - acc: 0.9650 - val_loss: 0.0983 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09729\n",
      "Test Accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "# Decrease the number of nodes in hidden layers to see if the model underfits\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.underfit.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing nodes from hidden layers appears to have led to mild underfitting, as seen by the increase in validation and training loss, and worsening of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 932,362\n",
      "Trainable params: 932,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.3073 - acc: 0.9048 - val_loss: 0.1431 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14309, saving model to mnist.model.overfit2.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.1254 - acc: 0.9628 - val_loss: 0.1137 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14309 to 0.11367, saving model to mnist.model.overfit2.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0922 - acc: 0.9727 - val_loss: 0.1199 - val_acc: 0.9682\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11367\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.0779 - acc: 0.9782 - val_loss: 0.0948 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11367 to 0.09479, saving model to mnist.model.overfit2.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0641 - acc: 0.9820 - val_loss: 0.0938 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09479 to 0.09378, saving model to mnist.model.overfit2.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0587 - acc: 0.9837 - val_loss: 0.1205 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09378\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.0562 - acc: 0.9852 - val_loss: 0.1147 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09378\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.0495 - acc: 0.9869 - val_loss: 0.1202 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09378\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.0505 - acc: 0.9871 - val_loss: 0.1250 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09378\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0473 - acc: 0.9881 - val_loss: 0.1061 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09378\n",
      "Test Accuracy: 98.09%\n"
     ]
    }
   ],
   "source": [
    "# Increase the number of hidden layers to see if the model overfits\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.overfit2.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional hidden layer seems to have worsened our validation loss marginally, this indicates overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.3016 - acc: 0.9139 - val_loss: 0.1461 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14605, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1343 - acc: 0.9597 - val_loss: 0.1074 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14605 to 0.10738, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0941 - acc: 0.9714 - val_loss: 0.0913 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10738 to 0.09134, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0713 - acc: 0.9790 - val_loss: 0.0835 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09134 to 0.08347, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0584 - acc: 0.9823 - val_loss: 0.0821 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08347 to 0.08209, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0479 - acc: 0.9855 - val_loss: 0.0817 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08209 to 0.08172, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0398 - acc: 0.9880 - val_loss: 0.0764 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08172 to 0.07638, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0333 - acc: 0.9897 - val_loss: 0.0761 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07638 to 0.07613, saving model to mnist.model.underfit2.hdf5\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0290 - acc: 0.9907 - val_loss: 0.0847 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07613\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0243 - acc: 0.9921 - val_loss: 0.0825 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07613\n",
      "Test Accuracy: 98.22%\n"
     ]
    }
   ],
   "source": [
    "# Decrease the number of hidden layers to see if the model overfits\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.underfit2.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing one layer has improved the overall model! Validation and training loss is lower, and training set accuracy is higher than our original model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.2541 - acc: 0.9215 - val_loss: 0.1206 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12058, saving model to mnist.model.overfit3.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.0948 - acc: 0.9706 - val_loss: 0.0929 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12058 to 0.09287, saving model to mnist.model.overfit3.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.0625 - acc: 0.9800 - val_loss: 0.1140 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09287\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.0454 - acc: 0.9868 - val_loss: 0.0886 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09287 to 0.08855, saving model to mnist.model.overfit3.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.0318 - acc: 0.9897 - val_loss: 0.0963 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.08855\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.0256 - acc: 0.9921 - val_loss: 0.1026 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.08855\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.0976 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.08855\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.0184 - acc: 0.9941 - val_loss: 0.1205 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.08855\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.1352 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08855\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.1464 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08855\n",
      "Test Accuracy: 97.77%\n"
     ]
    }
   ],
   "source": [
    "# Remove dropout layer to see evidence of overfit\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.overfit3.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy is slightly lower if we drop the Dropout layers, and validation loss is slightly worse. Hence this model is a little more overfit than our original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.4848 - acc: 0.8594 - val_loss: 0.3786 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37864, saving model to mnist.model.underfit3.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3945 - acc: 0.8889 - val_loss: 0.3461 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37864 to 0.34607, saving model to mnist.model.underfit3.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.3682 - acc: 0.8962 - val_loss: 0.3605 - val_acc: 0.8988\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34607\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3529 - acc: 0.9019 - val_loss: 0.2964 - val_acc: 0.9179\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34607 to 0.29640, saving model to mnist.model.underfit3.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3467 - acc: 0.9025 - val_loss: 0.3135 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29640\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.3392 - acc: 0.9041 - val_loss: 0.2951 - val_acc: 0.9197\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29640 to 0.29506, saving model to mnist.model.underfit3.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.3372 - acc: 0.9061 - val_loss: 0.3064 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29506\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3362 - acc: 0.9060 - val_loss: 0.3318 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29506\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3336 - acc: 0.9077 - val_loss: 0.3099 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29506\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3332 - acc: 0.9070 - val_loss: 0.2995 - val_acc: 0.9203\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29506\n",
      "Test Accuracy: 91.94%\n"
     ]
    }
   ],
   "source": [
    "# Remove ReLU to see if accuracy drops\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.underfit3.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the model is significantly worse if we remove ReLu. The default softmax activation just does not work well for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 13.5893 - acc: 0.1564 - val_loss: 12.8206 - val_acc: 0.2042\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.82058, saving model to mnist.model.underfit4.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 12.6276 - acc: 0.2163 - val_loss: 12.5193 - val_acc: 0.2231\n",
      "\n",
      "Epoch 00002: val_loss improved from 12.82058 to 12.51929, saving model to mnist.model.underfit4.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 11.9883 - acc: 0.2560 - val_loss: 11.7052 - val_acc: 0.2736\n",
      "\n",
      "Epoch 00003: val_loss improved from 12.51929 to 11.70522, saving model to mnist.model.underfit4.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 11.9811 - acc: 0.2565 - val_loss: 12.3170 - val_acc: 0.2348\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 11.70522\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 11.4132 - acc: 0.2915 - val_loss: 10.6476 - val_acc: 0.3393\n",
      "\n",
      "Epoch 00005: val_loss improved from 11.70522 to 10.64758, saving model to mnist.model.underfit4.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 10.7112 - acc: 0.3351 - val_loss: 10.3144 - val_acc: 0.3599\n",
      "\n",
      "Epoch 00006: val_loss improved from 10.64758 to 10.31436, saving model to mnist.model.underfit4.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 10.5054 - acc: 0.3481 - val_loss: 10.8398 - val_acc: 0.3273\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 10.31436\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 10.5203 - acc: 0.3471 - val_loss: 10.9625 - val_acc: 0.3197\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 10.31436\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 10.2613 - acc: 0.3631 - val_loss: 9.9354 - val_acc: 0.3834\n",
      "\n",
      "Epoch 00009: val_loss improved from 10.31436 to 9.93536, saving model to mnist.model.underfit4.hdf5\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 9.6061 - acc: 0.4037 - val_loss: 8.9993 - val_acc: 0.4416\n",
      "\n",
      "Epoch 00010: val_loss improved from 9.93536 to 8.99929, saving model to mnist.model.underfit4.hdf5\n",
      "Test Accuracy: 43.20%\n"
     ]
    }
   ],
   "source": [
    "# Remove image pre-processing step to see how it impacts model accuracy\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.underfit4.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pre-processing step is crucial to the model performance. Test accuracy takes a huge hit if we do not convert the pixel values to range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the preprocessing step\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.3256 - acc: 0.9018 - val_loss: 0.1480 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14800, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.1279 - acc: 0.9604 - val_loss: 0.1248 - val_acc: 0.9603\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14800 to 0.12482, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0895 - acc: 0.9725 - val_loss: 0.0886 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12482 to 0.08860, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0685 - acc: 0.9784 - val_loss: 0.0762 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08860 to 0.07615, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0544 - acc: 0.9830 - val_loss: 0.0821 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07615\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0436 - acc: 0.9869 - val_loss: 0.0727 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07615 to 0.07272, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0366 - acc: 0.9885 - val_loss: 0.0704 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07272 to 0.07037, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0286 - acc: 0.9908 - val_loss: 0.0703 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07037 to 0.07026, saving model to mnist.model.optimizer.hdf5\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0260 - acc: 0.9913 - val_loss: 0.0730 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07026\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0221 - acc: 0.9924 - val_loss: 0.0721 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07026\n",
      "Test Accuracy: 98.26%\n"
     ]
    }
   ],
   "source": [
    "# Try a different optimizer\n",
    "\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.optimizer.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy improves and validation loss decreases! We have a better model with the adadelta optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.5876 - acc: 0.8162 - val_loss: 0.2520 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25202, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.2307 - acc: 0.9292 - val_loss: 0.1907 - val_acc: 0.9411\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25202 to 0.19066, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1620 - acc: 0.9509 - val_loss: 0.1532 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19066 to 0.15315, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1217 - acc: 0.9624 - val_loss: 0.1128 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15315 to 0.11277, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0921 - acc: 0.9722 - val_loss: 0.1118 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11277 to 0.11184, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0761 - acc: 0.9763 - val_loss: 0.0864 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11184 to 0.08645, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0624 - acc: 0.9803 - val_loss: 0.0851 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08645 to 0.08512, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0493 - acc: 0.9847 - val_loss: 0.1043 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.08512\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0422 - acc: 0.9863 - val_loss: 0.0735 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08512 to 0.07351, saving model to mnist.model.batch.increase.hdf5\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0356 - acc: 0.9887 - val_loss: 0.0751 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07351\n",
      "Test Accuracy: 98.21%\n"
     ]
    }
   ],
   "source": [
    "# Increase batch size\n",
    "\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.batch.increase.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=1024, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing batch size degraded the accuracy of the model, and led to a slight overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 10s 210us/step - loss: 0.2419 - acc: 0.9278 - val_loss: 0.1228 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12280, saving model to mnist.model.batch.decrease.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.1308 - acc: 0.9646 - val_loss: 0.1434 - val_acc: 0.9663\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12280\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 10s 200us/step - loss: 0.1203 - acc: 0.9715 - val_loss: 0.1399 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12280\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 10s 200us/step - loss: 0.1106 - acc: 0.9753 - val_loss: 0.1497 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12280\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.1105 - acc: 0.9770 - val_loss: 0.1339 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12280\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.1015 - acc: 0.9793 - val_loss: 0.1555 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12280\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 10s 203us/step - loss: 0.1033 - acc: 0.9806 - val_loss: 0.1526 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12280\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.0972 - acc: 0.9822 - val_loss: 0.1623 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12280\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.0972 - acc: 0.9828 - val_loss: 0.1714 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12280\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.0944 - acc: 0.9831 - val_loss: 0.1656 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12280\n",
      "Test Accuracy: 97.78%\n"
     ]
    }
   ],
   "source": [
    "# Decrease batch size\n",
    "\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.batch.decrease.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3438 - acc: 0.9012 - val_loss: 0.1844 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18443, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.1594 - acc: 0.9535 - val_loss: 0.1275 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18443 to 0.12749, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.1126 - acc: 0.9666 - val_loss: 0.1013 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12749 to 0.10126, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0878 - acc: 0.9744 - val_loss: 0.0901 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10126 to 0.09011, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0718 - acc: 0.9791 - val_loss: 0.0856 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09011 to 0.08562, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0604 - acc: 0.9823 - val_loss: 0.0768 - val_acc: 0.9767\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08562 to 0.07676, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0513 - acc: 0.9852 - val_loss: 0.0724 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07676 to 0.07238, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0447 - acc: 0.9869 - val_loss: 0.0719 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07238 to 0.07194, saving model to mnist.model.bestest.hdf5\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0387 - acc: 0.9887 - val_loss: 0.0758 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07194\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0336 - acc: 0.9901 - val_loss: 0.0698 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.07194 to 0.06978, saving model to mnist.model.bestest.hdf5\n",
      "Test Accuracy: 98.07%\n"
     ]
    }
   ],
   "source": [
    "# Train possibly the best-est model, by combining the two models that did well before\n",
    "# Decrease the number of hidden layers to see if the model overfits\n",
    "model = Sequential()\n",
    "# MLP only takes vectors as input; vectorize all matrices\n",
    "# Flatten layer takes the matrix and converts to a vector\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.bestest.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print 'Test Accuracy: %.2f%%' % accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
